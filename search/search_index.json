{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p>Agin is a one-stop machine learning solution designed to streamline your ML workflows with easy-to-use utilities and a flexible structure.</p>"},{"location":"#features","title":"Features","text":"<ul> <li>Modular design for efficient machine learning workflows.</li> <li>Easy integration with existing Python projects.</li> <li>Simple and intuitive API.</li> </ul>"},{"location":"#installation","title":"Installation","text":"<p>To install the package, use:</p> <pre><code>pip install AginPy\n</code></pre>"},{"location":"#components","title":"Components","text":"<p>For detailed information, see the following sections:</p> <ul> <li>Classification</li> <li>Regression</li> <li>Utils</li> </ul>"},{"location":"#maintained-by","title":"Maintained By","text":"<p>Agin is maintained by the following individuals:</p> Agnik Bakshi         Maintainer and Contributor @AginPy      Indranjana Chatterjee         Maintainer and Contributor @AginPy      <p>For inquiries, please contact the maintainers through the GitHub repository or email support.</p>"},{"location":"classification/","title":"Classification","text":"<p>The <code>classification</code> module contains implementations of classification models. Currently, the package supports:</p> <ul> <li>Logistic Regression</li> <li>Naive Bayes Classifier</li> <li>K-Nearest Neighbors (KNN) Classifier</li> <li>Linear SVM Classifier</li> <li>Non-Linear SVM Classifier</li> <li>Decision Tree Classifier</li> <li>Random Forest Classifier</li> </ul>"},{"location":"classification/#logistic-regression","title":"Logistic Regression","text":"<p>The <code>LogisticRegression</code> class provides methods to train a logistic regression model using gradient descent with optional regularization, make predictions, and evaluate the model's performance. It can be imported directly from <code>agin</code> or from <code>agin.regression</code>.</p>"},{"location":"classification/#usage","title":"Usage","text":"<p>The <code>LogisticRegression</code> class can be imported directly from the <code>agin</code> package or from the <code>agin.regression</code> module:</p> <pre><code>from agin import LogisticRegression\n# or\nfrom agin.regression import LogisticRegression\n</code></pre>"},{"location":"classification/#example","title":"Example","text":"<pre><code># Option 1: Importing directly from agin\nfrom agin import LogisticRegression\n\n# Option 2: Importing from agin.regression\nfrom agin.regression import LogisticRegression\n\n# Training data\nx_train = [[1, 2], [2, 3], [3, 4], [4, 5]]\ny_train = [0, 1, 0, 1]\n\n# Initialize the model\nmodel = LogisticRegression(regularization='l2', C=1.0, max_iter=100)\n\n# Fit the model\nmodel.fit(x_train, y_train)\n\n# Predict using the model\nx_test = [[5, 6], [6, 7]]\ny_pred = model.predict(x_test)\n\nprint(\"Predictions:\", y_pred)\n\n# Evaluate the model metrics\ny_test = [1, 0]\naccuracy, precision, recall, f1_score = model.metrics(y_pred, y_test)\nprint(\"Accuracy:\", accuracy)\nprint(\"Precision:\", precision)\nprint(\"Recall:\", recall)\nprint(\"F1 Score:\", f1_score)\n</code></pre>"},{"location":"classification/#methods","title":"Methods","text":""},{"location":"classification/#fitx_train-y_train-epochsnone-learning_rate01-batch_size32","title":"<code>fit(x_train, y_train, epochs=None, learning_rate=0.1, batch_size=32)</code>","text":"<ul> <li>Trains the logistic regression model using gradient descent.</li> <li>Args:<ul> <li><code>x_train</code> (numpy.ndarray or pandas.DataFrame): Training feature data.</li> <li><code>y_train</code> (numpy.ndarray or pandas.DataFrame): Target labels.</li> <li><code>epochs</code> (int): Number of epochs for training. Default is the value of <code>max_iter</code>.</li> <li><code>learning_rate</code> (float): Learning rate for gradient updates. Default is 0.1.</li> <li><code>batch_size</code> (int): Size of batches for mini-batch gradient descent. Default is 32.</li> </ul> </li> <li>Returns: The trained LogisticRegression model.</li> </ul>"},{"location":"classification/#predict_probabilitiesx","title":"<code>predict_probabilities(x)</code>","text":"<ul> <li>Predicts probabilities for the given feature data.</li> <li>Args: <code>x</code> (numpy.ndarray or pandas.DataFrame): Feature data.</li> <li>Returns: numpy.ndarray of predicted probabilities.</li> </ul>"},{"location":"classification/#predictx-threshold05","title":"<code>predict(x, threshold=0.5)</code>","text":"<ul> <li>Predicts class labels for the given feature data.</li> <li>Args:<ul> <li><code>x</code> (numpy.ndarray or pandas.DataFrame): Feature data.</li> <li><code>threshold</code> (float): Threshold for converting probabilities to binary class labels. Default is 0.5.</li> </ul> </li> <li>Returns: numpy.ndarray of predicted class labels.</li> </ul>"},{"location":"classification/#metricsy_pred-y_test","title":"<code>metrics(y_pred, y_test)</code>","text":"<ul> <li>Computes various evaluation metrics for classification.</li> <li>Args:<ul> <li><code>y_pred</code> (numpy.ndarray): Predicted labels.</li> <li><code>y_test</code> (numpy.ndarray): True labels.</li> </ul> </li> <li>Returns: Tuple containing accuracy, precision, recall, and F1-score.</li> </ul>"},{"location":"classification/#parameters","title":"Parameters","text":"<ul> <li><code>regularization</code> (str): Type of regularization ('l1', 'l2', 'elasticnet' or None). Default is 'l2'.</li> <li><code>C</code> (float): Inverse of regularization strength. Smaller values indicate stronger regularization. Default is 1.0.</li> <li><code>max_iter</code> (int): Maximum number of iterations for optimization. Default is 100.</li> <li><code>tol</code> (float): Tolerance for stopping criteria. Default is 1e-4.</li> <li><code>class_weight</code> (dict or 'balanced' or 'unbalanced'): Weights associated with classes. If 'balanced', class weights are computed inversely proportional to class frequencies. Default is None.</li> <li><code>random_state</code> (int): Seed for random number generation to ensure reproducibility. Default is None.</li> <li><code>l1_ratio</code> (float): The mixing parameter for elasticnet regularization. l1_ratio=1 corresponds to l1, while l1_ratio=0 corresponds to l2. Default is 0.5.</li> </ul>"},{"location":"classification/#attributes","title":"Attributes","text":"<ul> <li><code>weights</code> (numpy.ndarray): Model coefficients for features.</li> <li><code>bias</code> (float): Model intercept term.</li> <li><code>loss</code> (list): Training loss history.</li> <li><code>train_acc</code> (list): Training accuracy history.</li> </ul>"},{"location":"classification/#notes","title":"Notes","text":"<ul> <li>The model supports three types of regularization: L1 (Lasso), L2 (Ridge), and Elastic Net.</li> <li>Class weights can be automatically computed using the 'balanced' option for imbalanced datasets.</li> <li>The model uses mini-batch gradient descent for optimization, with customizable batch sizes.</li> <li>Early stopping is implemented based on the tolerance parameter.</li> <li>Features are automatically scaled using MinMaxScaler during training and prediction.</li> </ul>"},{"location":"classification/#naive-bayes-classifier","title":"Naive Bayes Classifier","text":"<p>The <code>NaiveBayesClassifier</code> class provides methods to train a Naive Bayes model, make predictions, and evaluate its performance. It can be imported directly from <code>agin</code> or from <code>agin.classification</code>.</p>"},{"location":"classification/#usage_1","title":"Usage","text":"<p>The <code>NaiveBayesClassifier</code> class can be imported directly from the <code>agin</code> package or from the <code>agin.classification</code> module:</p> <pre><code>from agin import NaiveBayesClassifier\n# or\nfrom agin.classification import NaiveBayesClassifier\n</code></pre>"},{"location":"classification/#example_1","title":"Example","text":"<pre><code># Option 1: Importing directly from agin\nfrom agin import NaiveBayesClassifier\n\n# Option 2: Importing from agin.classification\nfrom agin.classification import NaiveBayesClassifier\n\n# Training data\nx_train = [[1, 2], [2, 2], [3, 1], [4, 1]]\ny_train = ['Yes', 'No', 'Yes', 'No']\n\n# Initialize the model\nmodel = NaiveBayesClassifier()\n\n# Fit the model\nmodel.fit(x_train, y_train)\n\n# Predict using the model\nx_test = [[2, 2], [3, 1]]\ny_pred = model.predict(x_test)\n\nprint(\"Predictions:\", y_pred)\n\n# Evaluate the model metrics\ny_test = ['No', 'Yes']\naccuracy, precision, recall, f1_score = model.metrics(y_pred, y_test)\nprint(\"Accuracy:\", accuracy)\nprint(\"Precision:\", precision)\nprint(\"Recall:\", recall)\nprint(\"F1 Score:\", f1_score)\n</code></pre>"},{"location":"classification/#methods_1","title":"Methods","text":""},{"location":"classification/#fitx_train-y_train","title":"<code>fit(x_train, y_train)</code>","text":"<ul> <li>Trains the Naive Bayes model by calculating class probabilities and feature likelihoods.</li> <li>Args:<ul> <li><code>x_train</code> (list or numpy.ndarray): A 2D array containing the training data for independent variables.</li> <li><code>y_train</code> (list or numpy.ndarray): A 1D array containing the true class labels for the dependent variable.</li> </ul> </li> <li>Returns: None. Updates the model's class probabilities and feature likelihoods.</li> </ul>"},{"location":"classification/#predictx_test","title":"<code>predict(x_test)</code>","text":"<ul> <li>Predicts the class label for each sample in the test data using the trained Naive Bayes model.</li> <li>Args:<ul> <li><code>x_test</code> (list or numpy.ndarray): A 2D array containing test data for independent variables.</li> </ul> </li> <li>Returns: numpy.ndarray of predicted class labels.</li> </ul>"},{"location":"classification/#metricsy_pred-y_test_1","title":"<code>metrics(y_pred, y_test)</code>","text":"<ul> <li>Calculates the accuracy, precision, recall, and F1 score of the Naive Bayes classifier.</li> <li>Args:<ul> <li><code>y_pred</code> (list or numpy.ndarray): A 1D array containing the predicted class labels from the model.</li> <li><code>y_test</code> (list or numpy.ndarray): A 1D array containing the true class labels for the dependent variable.</li> </ul> </li> <li>Returns: Tuple containing accuracy, precision, recall, and F1-score.</li> </ul>"},{"location":"classification/#k-nearest-neighbors-knn-classifier","title":"K-Nearest Neighbors (KNN) Classifier","text":"<p>The <code>KNNClassifier</code> class implements the K-Nearest Neighbors algorithm for classification. It calculates the distances between test samples and training samples to identify the <code>k</code> nearest neighbors and predict labels using majority or weighted voting.</p>"},{"location":"classification/#usage_2","title":"Usage","text":"<p>The <code>KNNClassifier</code> class can be imported directly from the <code>agin</code> package or from the <code>agin.classification</code> module:</p> <pre><code>from agin import KNNClassifier\n# or\nfrom agin.classification import KNNClassifier\n</code></pre>"},{"location":"classification/#example_2","title":"Example","text":"<pre><code># Option 1: Importing directly from agin\nfrom agin import KNNClassifier\n\n# Option 2: Importing from agin.classification\nfrom agin.classification import KNNClassifier\n\n# Training data\nx_train = [[1, 2], [2, 3], [3, 4], [4, 5]]\ny_train = [0, 1, 0, 1]\n\n# Initialize the model\nmodel = KNNClassifier(n_neighbors=3, weights='distance', metric='euclidean')\n\n# Fit the model\nmodel.fit(x_train, y_train)\n\n# Predict using the model\nx_test = [[5, 6], [6, 7]]\ny_pred = model.predict(x_test)\n\nprint(\"Predictions:\", y_pred)\n\n# Evaluate the model metrics\ny_test = [1, 0]\naccuracy, precision, recall, f1_score = model.metrics(y_pred, y_test)\nprint(\"Accuracy:\", accuracy)\nprint(\"Precision:\", precision)\nprint(\"Recall:\", recall)\nprint(\"F1 Score:\", f1_score)\n</code></pre>"},{"location":"classification/#methods_2","title":"Methods","text":""},{"location":"classification/#fitx_train-y_train_1","title":"<code>fit(x_train, y_train)</code>","text":"<ul> <li>Stores the training data for use in distance calculations and predictions.</li> <li>Args:<ul> <li><code>x_train</code> (list or numpy.ndarray): A 2D array containing the training data for independent variables.</li> <li><code>y_train</code> (list or numpy.ndarray): A 1D array containing the true class labels for the dependent variable.</li> </ul> </li> <li>Returns: None. Updates the model with training data.</li> </ul>"},{"location":"classification/#predictx_test_1","title":"<code>predict(x_test)</code>","text":"<ul> <li>Predicts the class label for each sample in the test data.</li> <li>Args:<ul> <li><code>x_test</code> (list or numpy.ndarray): A 2D array containing the test data for independent variables.</li> </ul> </li> <li>Returns: numpy.ndarray of predicted class labels.</li> </ul>"},{"location":"classification/#metricsy_pred-y_test_2","title":"<code>metrics(y_pred, y_test)</code>","text":"<ul> <li>Calculates the accuracy, precision, recall, and F1 score of the KNN classifier.</li> <li>Args:<ul> <li><code>y_pred</code> (list or numpy.ndarray): A 1D array containing the predicted class labels from the model.</li> <li><code>y_test</code> (list or numpy.ndarray): A 1D array containing the true class labels for the dependent variable.</li> </ul> </li> <li>Returns: Tuple containing accuracy, precision, recall, and F1-score.</li> </ul>"},{"location":"classification/#linear-svm-classifier","title":"Linear SVM Classifier","text":"<p>The <code>LinearSVMClassifier</code> class implements the Linear Support Vector Machine (SVM) algorithm for classification tasks. It separates classes by finding the hyperplane that maximizes the margin between them.</p>"},{"location":"classification/#usage_3","title":"Usage","text":"<p>The <code>LinearSVMClassifier</code> class can be imported directly from <code>agin</code> or from <code>agin.classification</code>.</p> <pre><code>from agin import LinearSVMClassifier\n# or\nfrom agin.classification import LinearSVMClassifier\n</code></pre>"},{"location":"classification/#example_3","title":"Example","text":"<pre><code># Option 1: Importing directly from agin\nfrom agin import LinearSVMClassifier\n\n# Option 2: Importing from agin.classification\nfrom agin.classification import LinearSVMClassifier\n\n# Training data\nx_train = [[1, 2], [2, 3], [3, 4], [4, 5]]\ny_train = [0, 1, 0, 1]\n\n# Initialize the model\nmodel = LinearSVMClassifier(C=1.0, max_iter=100)\n\n# Fit the model\nmodel.fit(x_train, y_train)\n\n# Predict using the model\nx_test = [[5, 6], [6, 7]]\ny_pred = model.predict(x_test)\n\nprint(\"Predictions:\", y_pred)\n\n# Evaluate the model metrics\ny_test = [1, 0]\naccuracy, precision, recall, f1_score = model.metrics(y_pred, y_test)\nprint(\"Accuracy:\", accuracy)\nprint(\"Precision:\", precision)\nprint(\"Recall:\", recall)\nprint(\"F1 Score:\", f1_score)\n</code></pre>"},{"location":"classification/#methods_3","title":"Methods","text":""},{"location":"classification/#fitx_train-y_train_2","title":"<code>fit(x_train, y_train)</code>","text":"<ul> <li>Trains the SVM model by finding the optimal hyperplane.</li> <li>Args:<ul> <li><code>x_train</code> (numpy.ndarray or pandas.DataFrame): Training feature data.</li> <li><code>y_train</code> (numpy.ndarray or pandas.DataFrame): Target labels.</li> </ul> </li> <li>Returns: The trained LinearSVMClassifier model.</li> </ul>"},{"location":"classification/#predictx","title":"<code>predict(x)</code>","text":"<ul> <li>Predicts class labels for the given</li> </ul> <p>feature data.    - Args:      - <code>x</code> (numpy.ndarray or pandas.DataFrame): Feature data.    - Returns: numpy.ndarray of predicted class labels.</p>"},{"location":"classification/#metricsy_pred-y_test_3","title":"<code>metrics(y_pred, y_test)</code>","text":"<ul> <li>Calculates the accuracy, precision, recall, and F1 score of the KNN classifier.</li> <li>Args:<ul> <li><code>y_pred</code> (list or numpy.ndarray): A 1D array containing the predicted class labels from the model.</li> <li><code>y_test</code> (list or numpy.ndarray): A 1D array containing the true class labels for the dependent variable.</li> </ul> </li> <li>Returns: Tuple containing accuracy, precision, recall, and F1-score.</li> </ul>"},{"location":"classification/#linear-svm-classifier_1","title":"Linear SVM Classifier","text":"<p>The <code>LinearSVMClassifier</code> class implements the Linear Support Vector Machine (SVM) algorithm for classification tasks. It separates classes by finding the hyperplane that maximizes the margin between them.</p>"},{"location":"classification/#usage_4","title":"Usage","text":"<p>The <code>LinearSVMClassifier</code> class can be imported directly from <code>agin</code> or from <code>agin.classification</code>.</p> <pre><code>from agin import LinearSVMClassifier\n# or\nfrom agin.classification import LinearSVMClassifier\n</code></pre>"},{"location":"classification/#example_4","title":"Example","text":"<pre><code># Option 1: Importing directly from agin\nfrom agin import LinearSVMClassifier\n\n# Option 2: Importing from agin.classification\nfrom agin.classification import LinearSVMClassifier\n\n# Training data\nx_train = [[1, 2], [2, 3], [3, 4], [4, 5]]\ny_train = [0, 1, 0, 1]\n\n# Initialize the model\nmodel = LinearSVMClassifier(C=1.0, max_iter=100)\n\n# Fit the model\nmodel.fit(x_train, y_train)\n\n# Predict using the model\nx_test = [[5, 6], [6, 7]]\ny_pred = model.predict(x_test)\n\nprint(\"Predictions:\", y_pred)\n\n# Evaluate the model metrics\ny_test = [1, 0]\naccuracy, precision, recall, f1_score = model.metrics(y_pred, y_test)\nprint(\"Accuracy:\", accuracy)\nprint(\"Precision:\", precision)\nprint(\"Recall:\", recall)\nprint(\"F1 Score:\", f1_score)\n</code></pre>"},{"location":"classification/#methods_4","title":"Methods","text":""},{"location":"classification/#fitx_train-y_train_3","title":"<code>fit(x_train, y_train)</code>","text":"<ul> <li>Trains the SVM model by finding the optimal hyperplane.</li> <li>Args:<ul> <li><code>x_train</code> (numpy.ndarray or pandas.DataFrame): Training feature data.</li> <li><code>y_train</code> (numpy.ndarray or pandas.DataFrame): Target labels.</li> </ul> </li> <li>Returns: The trained LinearSVMClassifier model.</li> </ul>"},{"location":"classification/#predictx_1","title":"<code>predict(x)</code>","text":"<ul> <li>Predicts class labels for the given feature data.</li> <li>Args: <code>x</code> (numpy.ndarray or pandas.DataFrame): Feature data.</li> <li>Returns: numpy.ndarray of predicted class labels.</li> </ul>"},{"location":"classification/#metricsy_pred-y_test_4","title":"<code>metrics(y_pred, y_test)</code>","text":"<ul> <li>Computes various evaluation metrics for classification.</li> <li>Args:<ul> <li><code>y_pred</code> (numpy.ndarray): Predicted labels.</li> <li><code>y_test</code> (numpy.ndarray): True labels.</li> </ul> </li> <li>Returns: Tuple containing accuracy, precision, recall, and F1-score.</li> </ul>"},{"location":"classification/#non-linear-svm-classifier","title":"Non-Linear SVM Classifier","text":"<p>The <code>NonLinearSVMClassifier</code> class implements the Support Vector Machine (SVM) algorithm using a non-linear kernel (e.g., RBF, polynomial) to classify data that cannot be separated by a straight line. The class supports multiple kernel types and hyperparameter tuning.</p>"},{"location":"classification/#usage_5","title":"Usage","text":"<p>The <code>NonLinearSVMClassifier</code> class can be imported directly from <code>agin</code> or from <code>agin.classification</code>.</p> <pre><code>from agin import NonLinearSVMClassifier\n# or\nfrom agin.classification import NonLinearSVMClassifier\n</code></pre>"},{"location":"classification/#example_5","title":"Example","text":"<pre><code># Option 1: Importing directly from agin\nfrom agin import NonLinearSVMClassifier\n\n# Option 2: Importing from agin.classification\nfrom agin.classification import NonLinearSVMClassifier\n\n# Training data\nx_train = [[1, 2], [2, 3], [3, 4], [4, 5]]\ny_train = [0, 1, 0, 1]\n\n# Initialize the model\nmodel = NonLinearSVMClassifier(kernel='rbf', C=1.0, gamma='scale', max_iter=100)\n\n# Fit the model\nmodel.fit(x_train, y_train)\n\n# Predict using the model\nx_test = [[5, 6], [6, 7]]\ny_pred = model.predict(x_test)\n\nprint(\"Predictions:\", y_pred)\n\n# Evaluate the model metrics\ny_test = [1, 0]\naccuracy, precision, recall, f1_score = model.metrics(y_pred, y_test)\nprint(\"Accuracy:\", accuracy)\nprint(\"Precision:\", precision)\nprint(\"Recall:\", recall)\nprint(\"F1 Score:\", f1_score)\n</code></pre>"},{"location":"classification/#methods_5","title":"Methods","text":""},{"location":"classification/#fitx_train-y_train_4","title":"<code>fit(x_train, y_train)</code>","text":"<ul> <li>Trains the non-linear SVM model using the specified kernel.</li> <li>Args:<ul> <li><code>x_train</code> (numpy.ndarray or pandas.DataFrame): Training feature data.</li> <li><code>y_train</code> (numpy.ndarray or pandas.DataFrame): Target labels.</li> </ul> </li> <li>Returns: The trained NonLinearSVMClassifier model.</li> </ul>"},{"location":"classification/#predictx_2","title":"<code>predict(x)</code>","text":"<ul> <li>Predicts class labels for the given feature data.</li> <li>Args:<ul> <li><code>x</code> (numpy.ndarray or pandas.DataFrame): Feature data.</li> </ul> </li> <li>Returns: numpy.ndarray of predicted class labels.</li> </ul>"},{"location":"classification/#metricsy_pred-y_test_5","title":"<code>metrics(y_pred, y_test)</code>","text":"<ul> <li>Computes various evaluation metrics for classification.</li> <li>Args:<ul> <li><code>y_pred</code> (numpy.ndarray): Predicted labels.</li> <li><code>y_test</code> (numpy.ndarray): True labels.</li> </ul> </li> <li>Returns: Tuple containing accuracy, precision, recall, and F1-score.</li> </ul>"},{"location":"classification/#parameters_1","title":"Parameters","text":"<ul> <li><code>kernel</code> (str): The kernel function to use ('linear', 'poly', 'rbf', 'sigmoid'). Default is 'rbf'.</li> <li><code>C</code> (float): Regularization parameter. Default is 1.0.</li> <li><code>gamma</code> (str or float): Kernel coefficient for 'rbf', 'poly', and 'sigmoid'. Default is 'scale'.</li> <li><code>degree</code> (int): Degree of the polynomial kernel function ('poly'). Default is 3.</li> <li><code>max_iter</code> (int): Maximum number of iterations for optimization. Default is 1000.</li> </ul>"},{"location":"classification/#decision-tree-classifier","title":"Decision Tree Classifier","text":"<p>The <code>DecisionTreeClassifier</code> class implements a decision tree for classification tasks. It splits the data based on feature thresholds to create a tree structure that classifies samples.</p>"},{"location":"classification/#usage_6","title":"Usage","text":"<p>The <code>DecisionTreeClassifier</code> class can be imported directly from <code>agin</code> or from <code>agin.classification</code>.</p> <pre><code>from agin import DecisionTreeClassifier\n# or\nfrom agin.classification import DecisionTreeClassifier\n</code></pre>"},{"location":"classification/#example_6","title":"Example","text":"<pre><code># Option 1: Importing directly from agin\nfrom agin import DecisionTreeClassifier\n\n# Option 2: Importing from agin.classification\nfrom agin.classification import DecisionTreeClassifier\n\n# Training data\nx_train = [[1, 2], [2, 3], [3, 4], [4, 5]]\ny_train = [0, 1, 0, 1]\n\n# Initialize the model\nmodel = DecisionTreeClassifier(max_depth=3, min_samples_split=2)\n\n# Fit the model\nmodel.fit(x_train, y_train)\n\n# Predict using the model\nx_test = [[5, 6], [6, 7]]\ny_pred = model.predict(x_test)\n\nprint(\"Predictions:\", y_pred)\n\n# Evaluate the model metrics\ny_test = [1, 0]\naccuracy, precision, recall, f1_score = model.metrics(y_pred, y_test)\nprint(\"Accuracy:\", accuracy)\nprint(\"Precision:\", precision)\nprint(\"Recall:\", recall)\nprint(\"F1 Score:\", f1_score)\n</code></pre>"},{"location":"classification/#methods_6","title":"Methods","text":""},{"location":"classification/#fitx_train-y_train_5","title":"<code>fit(x_train, y_train)</code>","text":"<ul> <li>Trains the decision tree model by recursively splitting the data based on feature thresholds.</li> <li>Args:<ul> <li><code>x_train</code> (list or numpy.ndarray): Training feature data.</li> <li><code>y_train</code> (list or numpy.ndarray): Target labels.</li> </ul> </li> <li>Returns: None. Updates the decision tree structure.</li> </ul>"},{"location":"classification/#predictx_test_2","title":"<code>predict(x_test)</code>","text":"<ul> <li>Predicts class labels for the given test data.</li> <li>Args:<ul> <li><code>x_test</code> (list or numpy.ndarray): Feature data for prediction.</li> </ul> </li> <li>Returns: numpy.ndarray of predicted class labels.</li> </ul>"},{"location":"classification/#metricsy_pred-y_test_6","title":"<code>metrics(y_pred, y_test)</code>","text":"<ul> <li>Computes various evaluation metrics for classification.</li> <li>Args:<ul> <li><code>y_pred</code> (numpy.ndarray): Predicted labels.</li> <li><code>y_test</code> (numpy.ndarray): True labels.</li> </ul> </li> <li>Returns: Tuple containing accuracy, precision, recall, and F1-score.</li> </ul>"},{"location":"classification/#parameters_2","title":"Parameters","text":"<ul> <li><code>max_depth</code> (int): The maximum depth of the tree. Default is None (unbounded depth).</li> <li><code>min_samples_split</code> (int): The minimum number of samples required to split an internal node. Default is 2.</li> <li><code>criterion</code> (str): The function to measure the quality of a split ('gini' or 'entropy'). Default is 'gini'.</li> <li><code>random_state</code> (int): Seed for random number generation to ensure reproducibility. Default is None.</li> </ul>"},{"location":"classification/#random-forest-classifier","title":"Random Forest Classifier","text":"<p>The <code>RandomForestClassifier</code> class implements an ensemble learning method that builds multiple decision trees during training and outputs the class that is the mode of the classes output by individual trees.</p>"},{"location":"classification/#usage_7","title":"Usage","text":"<p>The <code>RandomForestClassifier</code> class can be imported directly from <code>agin</code> or from the <code>agin.classification</code> module:</p> <pre><code>from agin import RandomForestClassifier\n# or\nfrom agin.classification import RandomForestClassifier\n</code></pre>"},{"location":"classification/#example_7","title":"Example","text":"<pre><code># Option 1: Importing directly from agin\nfrom agin import RandomForestClassifier\n\n# Option 2: Importing from agin.classification\nfrom agin.classification import RandomForestClassifier\n\n# Training data\nx_train = [[1, 2], [2, 3], [3, 4], [4, 5]]\ny_train = [0, 1, 0, 1]\n\n# Initialize the model\nmodel = RandomForestClassifier(n_estimators=10, max_depth=5, random_state=42)\n\n# Fit the model\nmodel.fit(x_train, y_train)\n\n# Predict using the model\nx_test = [[5, 6], [6, 7]]\ny_pred = model.predict(x_test)\n\nprint(\"Predictions:\", y_pred)\n\n# Evaluate the model metrics\ny_test = [1, 0]\naccuracy, precision, recall, f1_score = model.metrics(y_pred, y_test)\nprint(\"Accuracy:\", accuracy)\nprint(\"Precision:\", precision)\nprint(\"Recall:\", recall)\nprint(\"F1 Score:\", f1_score)\n</code></pre>"},{"location":"classification/#methods_7","title":"Methods","text":""},{"location":"classification/#fitx_train-y_train_6","title":"<code>fit(x_train, y_train)</code>","text":"<ul> <li>Trains the Random Forest model by building multiple decision trees using bootstrap sampling and aggregating their outputs.</li> <li>Args:<ul> <li><code>x_train</code> (list or numpy.ndarray): Training feature data.</li> <li><code>y_train</code> (list or numpy.ndarray): Target labels.</li> </ul> </li> <li>Returns: None. Updates the model with trained trees.</li> </ul>"},{"location":"classification/#predictx_test_3","title":"<code>predict(x_test)</code>","text":"<ul> <li>Predicts the class label for each sample in the test data by aggregating predictions from all decision trees.</li> <li>Args:<ul> <li><code>x_test</code> (list or numpy.ndarray): Feature data for prediction.</li> </ul> </li> <li>Returns: numpy.ndarray of predicted class labels.</li> </ul>"},{"location":"classification/#metricsy_pred-y_test_7","title":"<code>metrics(y_pred, y_test)</code>","text":"<ul> <li>Computes various evaluation metrics for classification.</li> <li>Args:<ul> <li><code>y_pred</code> (numpy.ndarray): Predicted labels.</li> <li><code>y_test</code> (numpy.ndarray): True labels.</li> </ul> </li> <li>Returns: Tuple containing accuracy, precision, recall, and F1-score.</li> </ul>"},{"location":"classification/#parameters_3","title":"Parameters","text":"<ul> <li><code>n_estimators</code> (int): The number of trees in the forest. Default is 100.</li> <li><code>max_depth</code> (int): The maximum depth of the trees. Default is None (nodes are expanded until all leaves are pure).</li> <li><code>min_samples_split</code> (int): The minimum number of samples required to split an internal node. Default is 2.</li> <li><code>min_samples_leaf</code> (int): The minimum number of samples required to be at a leaf node. Default is 1.</li> <li><code>random_state</code> (int): Seed for random number generation to ensure reproducibility. Default is None.</li> <li><code>criterion</code> (str): The function to measure the quality of a split ('gini' or 'entropy'). Default is 'gini'.</li> </ul>"},{"location":"classification/#attributes_1","title":"Attributes","text":"<ul> <li><code>trees</code> (list): A list of trained decision tree classifiers.</li> <li><code>feature_importances_</code> (numpy.ndarray): The feature importances computed as the mean and standard deviation of accumulation across trees.</li> </ul>"},{"location":"neural/","title":"Neural Network","text":"<p>The <code>neural_network</code> module contains implementations of neural network and deep learning models. Currently, the package supports: - Neural Network</p>"},{"location":"neural/#neural-network_1","title":"Neural Network","text":"<p>The <code>NeuralNetwork</code> class implements a basic multi-layer perceptron (MLP) model for multi-class classification using gradient descent. It utilizes the tanh activation function in the hidden layer and the softmax function in the output layer. The model is trained using forward propagation, backward propagation (backpropagation), and gradient descent. It can be imported directly from <code>agin</code> or from <code>agin.neural_network</code>.</p>"},{"location":"neural/#usage","title":"Usage","text":"<p>The <code>NeuralNetwork</code> class can be imported directly from the <code>agin</code> package or from the <code>agin.neural_network</code> module:</p> <pre><code>from agin import NeuralNetwork\n# or\nfrom agin.neural_network import NeuralNetwork\n</code></pre>"},{"location":"neural/#example","title":"Example","text":"<pre><code># Option 1: Importing directly from agin\nfrom agin import NeuralNetwork\n\n# Option 2: Importing from agin.regression\nfrom agin.neural_network import NeuralNetwork\n\n# Training data\nx_train = np.random.randn(3, 1000)\ny_train = np.eye(3)[np.random.choice(3, 1000)].T\n\nx_test = np.random.randn(3, 200)\ny_test = np.eye(3)[np.random.choice(3, 200)].T\n\n# Neural network parameters\nn_x = x_train.shape[0]\nn_h = 5\nn_y = y_train.shape[0]\n# Initialize the model\nmodel = NeuralNetwork(n_x, n_h, n_y, learning_rate=0.1, iterations=2000)\nmodel.fit(x_train, y_train)\n# Evaluate the model\naccuracy, precision, recall, f1_score = model.metrics(x_test, y_test)\nprint(f\"\\nAccuracy:  Custom: {accuracy:.2f}%\")\nprint(f\"Precision:  Custom: {precision:.2f}% \")\nprint(f\"Recall:  Custom: {recall:.2f}%\")\nprint(f\"F1:  Custom: {f1_score:.2f}%\")\n\n\n</code></pre>"},{"location":"neural/#methods","title":"Methods","text":""},{"location":"neural/#fitx_train-y_train","title":"<code>fit(x_train, y_train)</code>","text":"<ul> <li>Trains the neural network by adjusting the weights using backpropagation.</li> <li>Args:<ul> <li><code>x_train</code> (numpy.ndarray or pandas.DataFrame): Training feature data.</li> <li><code>y_train</code> (numpy.ndarray or pandas.DataFrame): Target labels.</li> </ul> </li> <li>Returns: None. The model's weights are updated during training.</li> </ul>"},{"location":"neural/#predictx","title":"<code>predict(x)</code>","text":"<ul> <li>Predicts class labels for the given feature data.</li> <li>Args:<ul> <li><code>x</code> (numpy.ndarray or pandas.DataFrame): Feature data.</li> </ul> </li> <li>Returns: numpy.ndarray of predicted class labels.</li> </ul>"},{"location":"neural/#metricsy_pred-y_test","title":"<code>metrics(y_pred, y_test)</code>","text":"<ul> <li>Computes various evaluation metrics for classification.</li> <li>Args:<ul> <li><code>y_pred</code> (numpy.ndarray): Predicted labels.</li> <li><code>y_test</code> (numpy.ndarray): True labels.</li> </ul> </li> <li>Returns: Tuple containing accuracy, precision, recall, and F1-score.</li> </ul>"},{"location":"neural/#parameters","title":"Parameters","text":"<ul> <li><code>layers</code> (list of int): List specifying the number of neurons in each layer, including input and output layers. Example: <code>[2, 4, 1]</code> for a 2-input, 4-hidden, and 1-output layer network.</li> <li><code>activation</code> (str): The activation function to use for the hidden layers ('relu', 'sigmoid', 'tanh'). Default is 'relu'.</li> <li><code>learning_rate</code> (float): The learning rate for gradient descent. Default is 0.01.</li> <li><code>epochs</code> (int): The number of training iterations. Default is 1000.</li> <li><code>batch_size</code> (int): The number of samples per gradient update. Default is 32.</li> </ul>"},{"location":"neural/#attributes","title":"Attributes","text":"<ul> <li><code>weights</code> (list of numpy.ndarray): The weights for each layer of the network.</li> <li><code>biases</code> (list of numpy.ndarray): The biases for each layer.</li> <li><code>loss_history</code> (list): The history of the loss function during training.</li> <li><code>accuracy_history</code> (list): The history of the accuracy during training.</li> </ul>"},{"location":"regression/","title":"Regression","text":"<p>The <code>regression</code> module contains implementations of regression models. Currently, the package supports:</p> <ul> <li>Linear Regression</li> <li>Multilinear Regression</li> <li>Ridge Regression</li> <li>Lasso Regression</li> <li>ElasticNet Regression</li> <li>Polynomial Regression</li> <li>K-Nearest Neighbors (KNN) Regression</li> <li>Decision Tree Regressor</li> <li>Random Forest Regressor</li> </ul>"},{"location":"regression/#linear-regression","title":"Linear Regression","text":"<p>The <code>LinearRegression</code> class provides methods to train a linear regression model, make predictions, and evaluate the model's performance. It can be imported directly from <code>agin</code> or from <code>agin.regression</code>.</p>"},{"location":"regression/#usage","title":"Usage","text":"<p>The <code>LinearRegression</code> class can be imported directly from the <code>agin</code> package or from the <code>agin.regression</code> module:</p> <pre><code>from agin import LinearRegression\n# or\nfrom agin.regression import LinearRegression\n</code></pre>"},{"location":"regression/#example","title":"Example","text":"<pre><code># Option 1: Importing directly from agin\nfrom agin import LinearRegression\n\n# Option 2: Importing from agin.regression\nfrom agin.regression import LinearRegression\n\n# Training data\nx_train = [1, 2, 3, 4, 5]\ny_train = [2, 4, 6, 8, 10]\n\n# Initialize the model\nmodel = LinearRegression()\n\n# Fit the model\nmodel.fit(x_train, y_train)\n\n# Predict using the model\nx_test = [0, 1, 2, 3, 4, 5]\ny_test = [1, 3, 5, 7, 9, 11]\n\ny_pred = model.predict(x_test)\n\nprint(\"Predictions:\", y_pred)\n\n# Evaluate the model metrics\nmse, r2 = model.metrics(y_pred, y_test)\nprint(\"Mean Squared Error:\", mse)\nprint(\"R2 Score:\", r2)\n</code></pre>"},{"location":"regression/#methods","title":"Methods","text":""},{"location":"regression/#fitx_train-y_train","title":"<code>fit(x_train, y_train)</code>","text":"<ul> <li>Trains the model based on the input data.</li> <li>Args: <code>x_train</code> (list or numpy.ndarray), <code>y_train</code> (list or numpy.ndarray)</li> <li>Returns: None</li> </ul>"},{"location":"regression/#predictx_test","title":"<code>predict(x_test)</code>","text":"<ul> <li>Predicts outputs for given input data.</li> <li>Args: <code>x_test</code> (list or numpy.ndarray)</li> <li>Returns: List of predicted values</li> </ul>"},{"location":"regression/#metricsy_pred-y_test","title":"<code>metrics(y_pred, y_test)</code>","text":"<ul> <li>Calculates evaluation metrics like Mean Squared Error (MSE) and R\u00b2 Score.</li> <li>Args: <code>y_pred</code> (list or numpy.ndarray), <code>y_test</code> (list or numpy.ndarray)</li> <li>Returns: Tuple containing MSE and R\u00b2 Score.</li> </ul>"},{"location":"regression/#multilinear-regression","title":"Multilinear Regression","text":"<p>The <code>MultilinearRegression</code> class provides methods to train a multilinear regression model, make predictions, and evaluate the model's performance. It can be imported directly from <code>agin</code> or from <code>agin.regression</code>.</p>"},{"location":"regression/#usage_1","title":"Usage","text":"<p>The <code>MultilinearRegression</code> class can be imported directly from the <code>agin</code> package or from the <code>agin.regression</code> module:</p> <pre><code>from agin import MultilinearRegression\n# or\nfrom agin.regression import MultilinearRegression\n</code></pre>"},{"location":"regression/#example_1","title":"Example","text":"<pre><code># Option 1: Importing directly from agin\nfrom agin import MultilinearRegression\n\n# Option 2: Importing from agin.regression\nfrom agin.regression import MultilinearRegression\n\n# Training data\nx_train = [[1, 2], [2, 3], [3, 4], [4, 5]]\ny_train = [3, 5, 7, 9]\n\n# Initialize the model\nmodel = MultilinearRegression()\n\n# Fit the model\nmodel.fit(x_train, y_train)\n\n# Predict using the model\nx_test = [[5, 6], [6, 7]]\n\ny_pred = model.predict(x_test)\n\nprint(\"Predictions:\", y_pred)\n\n# Evaluate the model metrics\ny_test = [11, 13]\nmse, r2 = model.metrics(y_pred, y_test)\nprint(\"Mean Squared Error:\", mse)\nprint(\"R2 Score:\", r2)\n</code></pre>"},{"location":"regression/#methods_1","title":"Methods","text":""},{"location":"regression/#fitx_train-y_train_1","title":"<code>fit(x_train, y_train)</code>","text":"<ul> <li>Trains the model using the Normal Equation.</li> <li>Args: <code>x_train</code> (list or numpy.ndarray), <code>y_train</code> (list or numpy.ndarray)</li> <li>Returns: None</li> </ul>"},{"location":"regression/#predictx_test_1","title":"<code>predict(x_test)</code>","text":"<ul> <li>Predicts outputs for given input data.</li> <li>Args: <code>x_test</code> (list or numpy.ndarray)</li> <li>Returns: List of predicted values</li> </ul>"},{"location":"regression/#metricsy_pred-y_test_1","title":"<code>metrics(y_pred, y_test)</code>","text":"<ul> <li>Calculates evaluation metrics like Mean Squared Error (MSE) and R\u00b2 Score.</li> <li>Args: <code>y_pred</code> (list or numpy.ndarray), <code>y_test</code> (list or numpy.ndarray)</li> <li>Returns: Tuple containing MSE and R\u00b2 Score.</li> </ul>"},{"location":"regression/#ridge-regression","title":"Ridge Regression","text":"<p>The <code>RidgeRegression</code> class provides methods to train a ridge regression model, make predictions, and evaluate the model's performance using L2 regularization. Unlike linear regression, which minimizes only the sum of squared residuals, ridge regression adds a penalty for large coefficients, helping to mitigate overfitting and improve predictions when multicollinearity or small sample sizes are present. It can be imported directly from <code>agin</code> or from <code>agin.regression</code>.</p>"},{"location":"regression/#usage_2","title":"Usage","text":"<p>The <code>RidgeRegression</code> class can be imported directly from the <code>agin</code> package or from the <code>agin.regression</code> module:</p> <pre><code>from agin import RidgeRegression\n# or\nfrom agin.regression import RidgeRegression\n</code></pre>"},{"location":"regression/#example_2","title":"Example","text":"<pre><code># Option 1: Importing directly from agin\nfrom agin import RidgeRegression\n\n# Option 2: Importing from agin.regression\nfrom agin.regression import RidgeRegression\n\n# Training data\nx_train = [1, 2, 3, 4, 5]\ny_train = [2, 4, 6, 8, 10]\n\n# Initialize the model with regularization parameter alpha\nmodel = RidgeRegression(alpha=1.0)\n\n# Fit the model\nmodel.fit(x_train, y_train)\n\n# Predict using the model\nx_test = [0, 1, 2, 3, 4, 5]\ny_test = [1, 3, 5, 7, 9, 11]\n\ny_pred = model.predict(x_test)\n\nprint(\"Predictions:\", y_pred)\n\n# Evaluate the model metrics\nmse, r2_score = model.metrics(y_pred, y_test)\nprint(\"Mean Squared Error:\", mse)\nprint(\"R2 Score:\", r2_score)\n</code></pre>"},{"location":"regression/#methods_2","title":"Methods","text":""},{"location":"regression/#fitx_train-y_train_2","title":"<code>fit(x_train, y_train)</code>","text":"<ul> <li>Trains the model using the closed-form solution for ridge regression.</li> <li>Args: <code>x_train</code> (list or numpy.ndarray), <code>y_train</code> (list or numpy.ndarray)</li> <li>Returns: None</li> </ul>"},{"location":"regression/#predictx_test_2","title":"<code>predict(x_test)</code>","text":"<ul> <li>Predicts outputs for given input data.</li> <li>Args: <code>x_test</code> (list or numpy.ndarray)</li> <li>Returns: List of predicted values</li> </ul>"},{"location":"regression/#metricsy_pred-y_test_2","title":"<code>metrics(y_pred, y_test)</code>","text":"<ul> <li>Calculates evaluation metrics like Mean Squared Error (MSE) and R\u00b2 Score.</li> <li>Args: <code>y_pred</code> (list or numpy.ndarray), <code>y_test</code> (list or numpy.ndarray)</li> <li>Returns: Tuple containing MSE and R\u00b2 Score.</li> </ul>"},{"location":"regression/#attributes","title":"Attributes","text":"<ul> <li><code>alpha</code>: Regularization parameter controlling the strength of the L2 penalty.</li> <li><code>slope</code>: Coefficients of the model.</li> <li><code>intercept</code>: Intercept (bias term) of the model.</li> </ul>"},{"location":"regression/#lasso-regression","title":"Lasso Regression","text":"<p>The <code>LassoRegression</code> class implements the Lasso regression model using the coordinate descent algorithm. It supports training the model, making predictions, and evaluating its performance. The model introduces L1 regularization, which encourages sparsity in the coefficients by penalizing the absolute values of the coefficients. This makes it particularly useful for feature selection.</p>"},{"location":"regression/#usage_3","title":"Usage","text":"<p>The <code>LassoRegression</code> class can be imported directly from the <code>agin</code> package or from the <code>agin.regression</code> module:</p> <pre><code>from agin import LassoRegression\n# or\nfrom agin.regression import LassoRegression\n</code></pre>"},{"location":"regression/#example_3","title":"Example","text":"<pre><code># Option 1: Importing directly from agin\nfrom agin import LassoRegression\n\n# Option 2: Importing from agin.regression\nfrom agin.regression import LassoRegression\n\n# Training data\nx_train = [1, 2, 3, 4, 5]\ny_train = [2.1, 4.2, 6.1, 8.2, 10.1]\n\n# Initialize the model with regularization parameter alpha\nmodel = LassoRegression(alpha=0.5, max_iter=1000, tol=1e-4)\n\n# Fit the model\nmodel.fit(x_train, y_train)\n\n# Predict using the model\nx_test = [0, 1, 2, 3, 4, 5]\ny_test = [1.1, 3.2, 5.1, 7.2, 9.1, 11.2]\n\ny_pred = model.predict(x_test)\n\nprint(\"Predictions:\", y_pred)\n\n# Evaluate the model metrics\nmse, r2_score = model.metrics(y_pred, y_test)\nprint(\"Mean Squared Error:\", mse)\nprint(\"R2 Score:\", r2_score)\n</code></pre>"},{"location":"regression/#methods_3","title":"Methods","text":""},{"location":"regression/#fitx_train-y_train_3","title":"<code>fit(x_train, y_train)</code>","text":"<ul> <li>Trains the model using the coordinate descent algorithm for Lasso regression.</li> <li>Args:<ul> <li><code>x_train</code> (list or numpy.ndarray): Training feature data (X values).</li> <li><code>y_train</code> (list or numpy.ndarray): Target data (Y values).</li> </ul> </li> <li>Returns: None</li> </ul>"},{"location":"regression/#predictx_test_3","title":"<code>predict(x_test)</code>","text":"<ul> <li>Predicts outputs for given input feature data.</li> <li>Args:<ul> <li><code>x_test</code> (list or numpy.ndarray): Test feature data (X values).</li> </ul> </li> <li>Returns: numpy.ndarray of predicted values.</li> </ul>"},{"location":"regression/#metricsy_pred-y_test_3","title":"<code>metrics(y_pred, y_test)</code>","text":"<ul> <li>Calculates evaluation metrics like Mean Squared Error (MSE) and R\u00b2 Score.</li> <li>Args:<ul> <li><code>y_pred</code> (list or numpy.ndarray): Predicted values.</li> <li><code>y_test</code> (list or numpy.ndarray): Actual values (ground truth).</li> </ul> </li> <li>Returns: Tuple containing:<ul> <li><code>MSE</code> (float): Mean Squared Error of the model.</li> <li><code>R\u00b2</code> (float): R-squared value, indicating the proportion of variance explained by the model.</li> </ul> </li> </ul>"},{"location":"regression/#attributes_1","title":"Attributes","text":"<ul> <li><code>alpha</code>: Regularization parameter that controls the strength of the L1 penalty.</li> <li><code>max_iter</code>: Maximum number of iterations for the optimization algorithm.</li> <li><code>tol</code>: Tolerance for convergence of the optimization.</li> <li><code>slope</code>: Coefficients of the model (excluding the intercept).</li> <li><code>intercept</code>: Intercept (bias term) of the model.</li> </ul>"},{"location":"regression/#elasticnet-regression","title":"ElasticNet Regression","text":"<p>The <code>ElasticNetRegression</code> class implements the Elastic Net model, which combines the strengths of both Ridge (L2 regularization) and Lasso (L1 regularization) regression. It is particularly useful when dealing with datasets that exhibit multicollinearity and when feature selection is desired.</p>"},{"location":"regression/#usage_4","title":"Usage","text":"<p>The <code>ElasticNetRegression</code> class can be imported directly from the <code>agin</code> package or from the <code>agin.regression</code> module:</p> <pre><code>from agin import ElasticNetRegression\n# or\nfrom agin.regression import ElasticNetRegression\n</code></pre>"},{"location":"regression/#example_4","title":"Example","text":"<pre><code># Option 1: Importing directly from agin\nfrom agin import ElasticNetRegression\n\n# Option 2: Importing from agin.regression\nfrom agin.regression import ElasticNetRegression\n\n# Training data\nx_train = [1, 2, 3, 4, 5]\ny_train = [2.5, 4.0, 6.5, 8.0, 10.5]\n\n# Initialize the model with alpha and l1_ratio\nmodel = ElasticNetRegression(alpha=1.0, l1_ratio=0.5)\n\n# Fit the model\nmodel.fit(x_train, y_train)\n\n# Predict using the model\nx_test = [0, 1, 2, 3, 4, 5]\ny_test = [1.0, 3.0, 5.0, 7.0, 9.0, 11.0]\n\ny_pred = model.predict(x_test)\n\nprint(\"Predictions:\", y_pred)\n\n# Evaluate the model metrics\nmse, r2_score = model.metrics(y_pred, y_test)\nprint(\"Mean Squared Error:\", mse)\nprint(\"R2 Score:\", r2_score)\n</code></pre>"},{"location":"regression/#methods_4","title":"Methods","text":""},{"location":"regression/#fitx_train-y_train_4","title":"<code>fit(x_train, y_train)</code>","text":"<ul> <li>Trains the model using a combination of L1 and L2 penalties.</li> <li>Args:<ul> <li><code>x_train</code> (list or numpy.ndarray): Training feature data (X values).</li> <li><code>y_train</code> (list or numpy.ndarray): Target data (Y values).</li> </ul> </li> <li>Returns: None</li> </ul>"},{"location":"regression/#predictx_test_4","title":"<code>predict(x_test)</code>","text":"<ul> <li>Predicts outputs for given input data.</li> <li>Args: <code>x_test</code> (list or numpy.ndarray)</li> <li>Returns: List of predicted values</li> </ul>"},{"location":"regression/#metricsy_pred-y_test_4","title":"<code>metrics(y_pred, y_test)</code>","text":"<ul> <li>Calculates evaluation metrics like Mean Squared Error (MSE) and R\u00b2 Score.</li> <li>Args:<ul> <li><code>y_pred</code> (list or numpy.ndarray): Predicted values.</li> <li><code>y_test</code> (list or numpy.ndarray): Actual values (ground truth).</li> </ul> </li> <li>Returns: Tuple containing MSE and R\u00b2 Score.</li> </ul>"},{"location":"regression/#attributes_2","title":"Attributes","text":"<ul> <li><code>alpha</code>: Regularization strength parameter. Higher values imply stronger regularization.</li> <li><code>l1_ratio</code>: The mixing ratio between L1 and L2 regularization. Ranges from 0 (Ridge regression) to 1 (Lasso regression).</li> <li><code>slope</code>: Coefficients of the model (excluding the intercept).</li> <li><code>intercept</code>: Intercept (bias term) of the model.</li> </ul>"},{"location":"regression/#polynomial-regression","title":"Polynomial Regression","text":"<p>The <code>PolynomialRegression</code> class provides methods to train a polynomial regression model of any degree, make predictions, and evaluate the model's performance. It can be imported directly from <code>agin</code> or from the <code>agin.regression</code>.</p>"},{"location":"regression/#usage_5","title":"Usage","text":"<p>The <code>PolynomialRegression</code> class can be imported directly from the <code>agin</code> package or from the <code>agin.regression</code> module:</p> <pre><code>from agin import PolynomialRegression\n# or\nfrom agin.regression import PolynomialRegression\n</code></pre>"},{"location":"regression/#example_5","title":"Example","text":"<pre><code># Option 1: Importing directly from agin\nfrom agin import PolynomialRegression\n\n# Option 2: Importing from agin.regression\nfrom agin.regression import PolynomialRegression\n\n# Training data\nx_train = [1, 2, 3, 4, 5]\ny_train = [2, 5, 10, 17, 26]\n\n# Initialize the model\nmodel = PolynomialRegression(degree=2)\n\n# Fit the model\nmodel.fit(x_train, y_train)\n\n# Predict using the model\nx_test = [6, 7, 8]\n\ny_pred = model.predict(x_test)\n\nprint(\"Predictions:\", y_pred)\n\n# Evaluate the model metrics\ny_test = [37, 50, 65]\nmse, r2 = model.metrics(y_pred, y_test)\nprint(\"Mean Squared Error:\", mse)\nprint(\"R2 Score:\", r2)\n</code></pre>"},{"location":"regression/#methods_5","title":"Methods","text":""},{"location":"regression/#fitx_train-y_train_5","title":"<code>fit(x_train, y_train)</code>","text":"<ul> <li>Trains the model using the Normal Equation for polynomial features.</li> <li>Args: <code>x_train</code> (list or numpy.ndarray), <code>y_train</code> (list or numpy.ndarray)</li> <li>Returns: None</li> </ul>"},{"location":"regression/#predictx_test_5","title":"<code>predict(x_test)</code>","text":"<ul> <li>Predicts outputs for given input data.</li> <li>Args: <code>x_test</code> (list or numpy.ndarray)</li> <li>Returns: List of predicted values</li> </ul>"},{"location":"regression/#metricsy_pred-y_test_5","title":"<code>metrics(y_pred, y_test)</code>","text":"<ul> <li>Calculates evaluation metrics like Mean Squared Error (MSE) and R\u00b2 Score.</li> <li>Args: <code>y_pred</code> (list or numpy.ndarray), <code>y_test</code> (list or numpy.ndarray)</li> <li>Returns: Tuple containing MSE and R\u00b2 Score.</li> </ul>"},{"location":"regression/#k-nearest-neighbors-knn-regression","title":"K-Nearest Neighbors (KNN) Regression","text":"<p>The <code>KNNRegressor</code> class provides a flexible implementation of the k-nearest neighbors algorithm for regression tasks. It supports both uniform and distance-based weighting schemes.</p>"},{"location":"regression/#usage_6","title":"Usage","text":"<p>The <code>KNNRegressor</code> class can be imported directly from the <code>agin</code> package or from the <code>agin.regression</code> module:</p> <pre><code>from agin import KNNRegressor\n# or\nfrom agin.regression import KNNRegressor\n</code></pre>"},{"location":"regression/#example_6","title":"Example","text":"<pre><code># Option 1: Importing directly from agin\nfrom agin import KNNRegressor\n\n# Option 2: Importing from agin.regression\nfrom agin.regression import KNNRegressor\n\n# Training data\nx_train = [[1.0], [2.0], [3.0], [4.0], [5.0]]\ny_train = [1.5, 2.5, 3.5, 4.5, 5.5]\n\n# Initialize the model\nmodel = KNNRegressor(n_neighbors=3, weights='distance', metric='euclidean')\n\n# Fit the model\nmodel.fit(x_train, y_train)\n\n# Predict using the model\nx_test = [[1.5], [2.5], [3.5]]\ny_pred = model.predict(x_test)\n\nprint(\"Predictions:\", y_pred)\n\n# Evaluate the model metrics\ny_test = [1.8, 2.8, 3.8]\nmse, r2 = model.metrics(y_pred, y_test)\nprint(\"Mean Squared Error:\", mse)\nprint(\"R2 Score:\", r2)\n</code></pre>"},{"location":"regression/#methods_6","title":"Methods","text":""},{"location":"regression/#fitx_train-y_train_6","title":"<code>fit(x_train, y_train)</code>","text":"<ul> <li>Stores the training data for use during prediction.</li> <li>Args: <code>x_train</code> (numpy.ndarray or pandas.DataFrame), <code>y_train</code> (numpy.ndarray or pandas.Series)</li> <li>Returns: The fitted regressor instance.</li> </ul>"},{"location":"regression/#predictx_test_6","title":"<code>predict(x_test)</code>","text":"<ul> <li>Predicts target values for given test samples.</li> <li>Args: <code>x_test</code> (numpy.ndarray or pandas.DataFrame)</li> <li>Returns: numpy.ndarray of predicted target values.</li> </ul>"},{"location":"regression/#metricsy_pred-y_test_6","title":"<code>metrics(y_pred, y_test)</code>","text":"<ul> <li>Calculates evaluation metrics like Mean Squared Error (MSE) and R\u00b2 Score.</li> <li>Args: <code>y_pred</code> (numpy.ndarray), <code>y_test</code> (numpy.ndarray)</li> <li>Returns: Tuple containing MSE and R\u00b2 Score.</li> </ul>"},{"location":"regression/#scorex_test-y_test","title":"<code>score(x_test, y_test)</code>","text":"<ul> <li>Computes the coefficient of determination (R\u00b2) score.</li> <li>Args: <code>x_test</code> (numpy.ndarray or pandas.DataFrame), <code>y_test</code> (numpy.ndarray or pandas.Series)</li> <li>Returns: R\u00b2 score as a float.</li> </ul>"},{"location":"regression/#decision-tree-regressor","title":"Decision Tree Regressor","text":"<p>The <code>DecisionTreeRegressor</code> class provides a regression model using a decision tree algorithm. It supports custom configurations like maximum depth, minimum samples per split, and minimum samples per leaf for controlling tree growth and preventing overfitting.</p>"},{"location":"regression/#usage_7","title":"Usage","text":"<p>The <code>DecisionTreeRegressor</code> class can be imported directly from the <code>agin</code> package or from the <code>agin.regression</code> module:</p> <pre><code>from agin import DecisionTreeRegressor\n# or\nfrom agin.regression import DecisionTreeRegressor\n</code></pre>"},{"location":"regression/#example_7","title":"Example","text":"<pre><code># Option 1: Importing directly from agin\nfrom agin import DecisionTreeRegressor\n\n# Option 2: Importing from agin.regression\nfrom agin.regression import DecisionTreeRegressor\n\n# Training data\nx_train = [[1], [2], [3], [4], [5]]\ny_train = [2.5, 3.5, 7.5, 9.0, 12.0]\n\n# Initialize the model\nmodel = DecisionTreeRegressor(max_depth=3, min_samples_split=2, random_state=42)\n\n# Fit the model\nmodel.fit(x_train, y_train)\n\n# Predict using the model\nx_test = [[1.5], [2.5], [3.5]]\ny_pred = model.predict(x_test)\n\nprint(\"Predictions:\", y_pred)\n\n# Evaluate the model metrics\ny_test = [3.0, 6.0, 8.5]\nmse, r2 = model.metrics(y_pred, y_test)\nprint(\"Mean Squared Error:\", mse)\nprint(\"R2 Score:\", r2)\n</code></pre>"},{"location":"regression/#methods_7","title":"Methods","text":""},{"location":"regression/#fitx_train-y_train_7","title":"<code>fit(x_train, y_train)</code>","text":"<ul> <li>Builds the decision tree from training data.</li> <li>Args: <ul> <li><code>x_train</code> (numpy.ndarray or pandas.DataFrame): Training features.</li> <li><code>y_train</code> (numpy.ndarray or pandas.Series): Training target values.</li> </ul> </li> <li>Returns: The fitted <code>DecisionTreeRegressor</code> instance.</li> </ul>"},{"location":"regression/#predictx_test_7","title":"<code>predict(x_test)</code>","text":"<ul> <li>Predicts target values for the given test data using the decision tree.</li> <li>Args: <ul> <li><code>x_test</code> (numpy.ndarray or pandas.DataFrame): Test data features.</li> </ul> </li> <li>Returns: numpy.ndarray of predicted target values.</li> </ul>"},{"location":"regression/#metricsy_pred-y_test_7","title":"<code>metrics(y_pred, y_test)</code>","text":"<ul> <li>Calculates evaluation metrics like Mean Squared Error (MSE) and R\u00b2 Score.</li> <li>Args: <ul> <li><code>y_pred</code> (numpy.ndarray): Predicted target values.</li> <li><code>y_test</code> (numpy.ndarray): True target values.</li> </ul> </li> <li>Returns: Tuple containing MSE and R\u00b2 Score.</li> </ul>"},{"location":"regression/#attributes_3","title":"Attributes","text":"<ul> <li><code>max_depth</code>: The maximum depth of the tree.</li> <li><code>min_samples_split</code>: Minimum number of samples required to split a node.</li> <li><code>min_samples_leaf</code>: Minimum number of samples required to form a leaf node.</li> <li><code>random_state</code>: Random seed for reproducibility.</li> <li><code>root</code>: The root node of the decision tree.</li> </ul>"},{"location":"regression/#random-forest-regressor","title":"Random Forest Regressor","text":"<p>The <code>RandomForestRegressor</code> class implements a regression model using an ensemble of decision trees. It provides methods to train the model, make predictions, and evaluate its performance. This regressor is robust to overfitting and can capture non-linear relationships effectively.</p>"},{"location":"regression/#usage_8","title":"Usage","text":"<p>The <code>RandomForestRegressor</code> class can be imported directly from the <code>agin</code> package or from the <code>agin.regression</code> module:</p> <pre><code>from agin import RandomForestRegressor\n# or\nfrom agin.regression import RandomForestRegressor\n</code></pre>"},{"location":"regression/#example_8","title":"Example","text":"<pre><code># Option 1: Importing directly from agin\nfrom agin import RandomForestRegressor\n\n# Option 2: Importing from agin.regression\nfrom agin.regression import RandomForestRegressor\n\n# Training data\nx_train = [[1], [2], [3], [4], [5]]\ny_train = [2.5, 3.5, 7.5, 9.0, 12.0]\n\n# Initialize the model\nmodel = RandomForestRegressor(n_estimators=100, max_depth=5, random_state=42)\n\n# Fit the model\nmodel.fit(x_train, y_train)\n\n# Predict using the model\nx_test = [[1.5], [2.5], [3.5]]\ny_pred = model.predict(x_test)\n\nprint(\"Predictions:\", y_pred)\n\n# Evaluate the model metrics\ny_test = [3.0, 6.0, 8.5]\nmse, r2 = model.metrics(y_pred, y_test)\nprint(\"Mean Squared Error:\", mse)\nprint(\"R2 Score:\", r2)\n</code></pre>"},{"location":"regression/#methods_8","title":"Methods","text":""},{"location":"regression/#fitx_train-y_train_8","title":"<code>fit(x_train, y_train)</code>","text":"<ul> <li>Trains the random forest model using the training data.</li> <li>Args: <ul> <li><code>x_train</code> (numpy.ndarray or pandas.DataFrame): Training features.</li> <li><code>y_train</code> (numpy.ndarray or pandas.Series): Training target values.</li> </ul> </li> <li>Returns: The fitted <code>RandomForestRegressor</code> instance.</li> </ul>"},{"location":"regression/#predictx_test_8","title":"<code>predict(x_test)</code>","text":"<ul> <li>Predicts target values for the given test data using the trained ensemble of trees.</li> <li>Args: <ul> <li><code>x_test</code> (numpy.ndarray or pandas.DataFrame): Test data features.</li> </ul> </li> <li>Returns: numpy.ndarray of predicted target values.</li> </ul>"},{"location":"regression/#metricsy_pred-y_test_8","title":"<code>metrics(y_pred, y_test)</code>","text":"<ul> <li>Calculates evaluation metrics like Mean Squared Error (MSE) and R\u00b2 Score.</li> <li>Args: <ul> <li><code>y_pred</code> (numpy.ndarray): Predicted target values.</li> <li><code>y_test</code> (numpy.ndarray): True target values.</li> </ul> </li> <li>Returns: Tuple containing MSE and R\u00b2 Score.</li> </ul>"},{"location":"regression/#attributes_4","title":"Attributes","text":"<ul> <li><code>n_estimators</code>: The number of trees in the forest (default: 100).</li> <li><code>max_depth</code>: The maximum depth of each tree (default: None).</li> <li><code>min_samples_split</code>: Minimum number of samples required to split a node (default: 2).</li> <li><code>min_samples_leaf</code>: Minimum number of samples required to form a leaf node (default: 1).</li> <li><code>random_state</code>: Random seed for reproducibility.</li> </ul>"},{"location":"utils/","title":"Utils","text":"<p>The <code>utils</code> module provides utility classes and methods to enhance your project.</p>"},{"location":"utils/#health-class","title":"Health Class","text":"<p>The <code>Health</code> class helps ensure the package is functioning correctly by providing a simple health-check mechanism.</p>"},{"location":"utils/#usage","title":"Usage","text":"<p>The <code>Health</code> class can be imported directly from the <code>agin</code> package or from the <code>agin.utils</code> module:</p> <pre><code>from agin import Health\n# or\nfrom agin.utils import Health\n</code></pre>"},{"location":"utils/#example","title":"Example","text":"<pre><code># Option 1: Importing directly from agin\nfrom agin import Health\n\n# Option 2: Importing from agin.utils\nfrom agin.utils import Health\n\n# Create a Health object\nhealth = Health(\"Excellent\")\n\n# Check the health status\nprint(health.check_health())  # Output: Health status: Excellent\n</code></pre>"},{"location":"utils/#methods","title":"Methods","text":""},{"location":"utils/#__init__statusgood","title":"<code>__init__(status=\"Good\")</code>","text":"<ul> <li>Initializes the <code>Health</code> class with a default status of \"Good\".</li> <li>Args: <code>status</code> (string)</li> </ul>"},{"location":"utils/#check_health","title":"<code>check_health()</code>","text":"<ul> <li>Returns the current health status.</li> <li>Args: None</li> <li>Returns: String representing the health status.</li> </ul>"}]}